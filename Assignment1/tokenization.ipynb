{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Choose a small corpus to build your tokenizer. This could be a collection of poems, short stories, news\n",
    "articles, or any other text your choice in any language.\n",
    "• Write a paragraph explaining why you chose this corpus. Is it useful for some particular task?"
   ],
   "id": "d0cba4c8c30aed25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. What is the fundamental difference between each tokenization method?",
   "id": "584fc4d076324cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The fundamental difference between WordPiece and Unigram tokenization is their approach to tokenization and vocabulary construction. Wordpiece strats from a small vocabulary that includes some special tokens and the initial vocabulary. The initial letter of each word is added to the vocabulary and the subsequent letters are added with a \"##\" prefix to indicate they are continuations of a word. By using a particular formula to compute the scores of each pair of tokens in the vocabulary, Wordpiece find the less useful pairs and merges them iteratively until the desired vocabulary size is reached. WordPiece for the tokenization saves only the vocabulary and not the merge rules learned. The tokenization is done by initially finding the logest subword in the vocabulary, then splits on it. Unigram in the other hand start from a large vocabulary and removes the less usefull tokens at each iteration until the desired vacabulary size is reached. To find the less usefull tokens Unigram computes the loss of the model given the entire vocabulary Subsequently for each symbol in the vocabulary it will compute the loss without that symbol. The difference between the two losses is the score of that symbol. The token with the lowest score is removed from the vocabulary.In the Unigram model the tokenization each token is independent from the previous one, so the text is generated by simply taking the most common token. To probability of a given toke is its frequency in the original corpus divided by the sum of all frequencies of all tokens in the vocabulary.\n",
    "\n"
   ],
   "id": "7ef5269fcaf12f9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. In the WordPiece script, what does the vocab_size variable do? What effect does it have on the output and the runtime to increase or decrease it by a factor of 10?\n",
    "The vocab_size variables refers to the size of the vocabulary that we want to reach at the end of the training process. The algorithm will perform merges until the vocabulary reach the desired size. By increasing the vocabulary size the computation time will increase but the generated vocabulary will bemore precise and more words can be represented directly. With a vocabulary size of 70 the sample phrase i choose is represented with 52 tokens and tokens are almost exclusively single characters. With a vocabulary size of 700 the same phrase is represented with 36 tokens mainly single words but there are also some tokens that are entire words and some are composed by two characters."
   ],
   "id": "100fcb8ce95e2e67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. In the Unigram/SentencePiece script, what does the percent_to_remove control? What happens if you change that value? Answer in general and for your corpus in particular.\n",
    "The percent_to_remove variable control the percentage of less useful tokens that are removed from the vocabulary at each iteration. Increasing this value will make the algorithm run faster, but it may also lead to suboptimal vocabulary since more tokens are removed at each step. For example, if we have a percentage_to_remove = 0.05, at each iteration the bottom 5% of the unigrams based on their probability scores are discarded.\n",
    "\n",
    "For my corpus for example if a set a percentage to remove equals to 0.05, the phrase \"This is a test phrase that will be tokenized\" is tokenized with a total of 36 tokens. The same phrase with a percentage to remove equals to 0.5 will be tokenized with 45 tokens. If i drop a really high percentage like 0.9 the code give an error. (thets because to work need a minimum number of tokens corespomding to single characters, by dropping 90% of the tokens we remove all the non single character tokens and also the single characte tokens)"
   ],
   "id": "f1d4176f1a252500"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Which tokenization method was the most useful for your corpus and why?\n",
    "For my corpus the most useful tokenization method was WordPiece. Wordpiece handle better the out of vocabulary words and the corpus i choose has a lot of them, thats probably why it perform better compared to unigram. Wordpiece also has a downside that it require mora data compared to unigram, so if the corpus is bigger the difference from the two models will be bigger for my corpus."
   ],
   "id": "5d394f93d9102a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4c67d8f17d099f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
