{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cA4f3BP_gZUC"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 1\n",
        "In these labs, we will be using the [Python 3](https://www.python.org/) programming language and the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). We will also be using Google Colab, a free service hosted by Google, which gives us access to a Linux machine that comes pre-installed with Python 3 and the NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMY8d4CJ46K2"
      },
      "source": [
        "## Using Google Colab\n",
        "Google Colab allows users to work with \"notebooks\", which consists of text cells and code cells. Text cells can be edited by double clicking them. A code cell can be executed by selecting it and pressing `Ctrl + Enter`. Code is shared between cells, meaning that you can, for example, create a variable in one cell and use it in another cell later on.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run all of the code cells in the notebook.\n",
        "\n",
        "## Resources\n",
        "* [The Python Standard Library](https://docs.python.org/3/library/index.html) - an overview of the built-in libraries in Python with a lot of examples.\n",
        "* [The Python Tutorial](https://docs.python.org/3/tutorial/index.html) - an official tutorial that gives a brief overview of the language.\n",
        "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) - a free book that offers a good introduction to the Python programming language to beginners.\n",
        "* [Natural Language Processing with Python](http://www.nltk.org/book/) - a free companion book to the NLTK toolkit.\n",
        "\n",
        "## Setting Python and the NLTK up on your own machine\n",
        "* [Python 3](https://realpython.com/installing-python/) - installation instructions\n",
        "* [NLTK](https://www.nltk.org/install.html) - installation instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19uyslsaYw4"
      },
      "source": [
        "## String methods in Python\n",
        "There are many ways to manipulate strings in Python. A full list of methods for the String class may be found in the [library reference](https://docs.python.org/3/library/stdtypes.html#string-methods)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzTEm3RCgZUU",
        "outputId": "2928123b-f28e-42d0-a482-975055155a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowercase: it was the best of times, it was the worst of times\n",
            "'times' count: 2\n",
            "First occurence of 'best': 11\n"
          ]
        }
      ],
      "source": [
        "a_string = \"It was the best of times, it was the worst of times\"\n",
        "\n",
        "print(\"Lowercase:\", a_string.lower())\n",
        "print(\"'times' count:\", a_string.count('times'))\n",
        "print(\"First occurence of 'best':\", a_string.find('best'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmTHYN-agZVa"
      },
      "source": [
        "## Lists, sets and built-in functions\n",
        "Lists and sets are two kinds of collections that can be used in Python.\n",
        "* A **list** is an *ordered* sequence of elements. Lists are enclosed with square brackets, and the elements are separated with a comma (e.g., `a_list = [\"This\", \"is\", \"a\", \"list\"]`).\n",
        "* A **set** is a collection of *unordered* and *unique* elements (meaning that it contains no duplicates). Sets are enclosed by curly braces, and the elements are separated by commas (e.g., `a_set = {\"This\", \"is\", \"a\", \"set\"}`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NWJM2rGgZVf",
        "outputId": "f81501bf-b92b-4236-dec8-bb9bd905f9f4",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split string: ['It', 'was', 'the', 'best', 'of', 'times,', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
            "Unique characters: {'a', 'b', 'o', 'r', 'I', 'm', 'w', 't', 'e', ' ', 'f', ',', 's', 'i', 'h'}\n",
            "No. of unique characters: 15\n",
            "Unique words: {'It', 'times,', 'worst', 'the', 'of', 'was', 'times', 'best', 'it'}\n",
            "No. of unique words: 9\n",
            "Longest word: times,\n"
          ]
        }
      ],
      "source": [
        "# Variables can be converted to lists and sets with the list() and set() functions\n",
        "char_list = list(a_string)\n",
        "char_set = set(a_string)\n",
        "\n",
        "# You can also split strings on certain characters to create a list of strings\n",
        "words = a_string.split()  # Splits on whitespaces by default\n",
        "print(\"Split string:\", words)\n",
        "\n",
        "# The len() and max() built-in methods\n",
        "print(\"Unique characters:\", char_set)\n",
        "print(\"No. of unique characters:\", len(char_set))\n",
        "print(\"Unique words:\", set(words))\n",
        "print(\"No. of unique words:\", len(set(words)))\n",
        "print(\"Longest word:\", max(words, key=len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUt68ulEpfuw"
      },
      "source": [
        "You can use the built-in function `help()` to quickly access documentation for a given object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En1YvSC4qHJ8",
        "outputId": "24f677e6-4750-4014-bab1-dbf319a69cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on built-in function max in module builtins:\n",
            "\n",
            "max(...)\n",
            "    max(iterable, *[, default=obj, key=func]) -> value\n",
            "    max(arg1, arg2, *args, *[, key=func]) -> value\n",
            "\n",
            "    With a single iterable argument, return its biggest item. The\n",
            "    default keyword-only argument specifies an object to return if\n",
            "    the provided iterable is empty.\n",
            "    With two or more arguments, return the largest argument.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_TWoEUgZVw"
      },
      "source": [
        "## Indices and slicing\n",
        "You can get characters and substrings at specific indexes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p4o5Lc_gZV2",
        "outputId": "2c4918cb-38df-47bd-b21e-6cfa51bcabc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "String: It was the best of times, it was the worst of times\n",
            "First character: I\n",
            "Last character: s\n",
            "Characters 11 to 14: best\n",
            "First 6 characters: It was\n",
            "Last 5 characters: times\n"
          ]
        }
      ],
      "source": [
        "print(\"String:\", a_string)\n",
        "print(\"First character:\", a_string[0])  # Indices in Python start at 0\n",
        "print(\"Last character:\", a_string[-1])  # A negative index starts counting from the end\n",
        "\n",
        "# We can get ranges of elements by slicing a list\n",
        "print(\"Characters 11 to 14:\", a_string[11:15])\n",
        "print(\"First 6 characters:\", a_string[:6])\n",
        "print(\"Last 5 characters:\", a_string[-5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wQ9qvZ2gZV_"
      },
      "source": [
        "## The Natural Language Toolkit (NLTK)\n",
        "The NLTK book, [Natural Language Processing with Python](http://www.nltk.org/book/), is an introduction to natural language processing in Python, using the NLTK library. [Chapter 1](http://www.nltk.org/book/ch01.html) is relevant to this lab. The NLTK comes with a lot of data, such as corpora and trained models. We can download this data with the `nltk.download()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txGWuokGgZWF",
        "outputId": "9c0a28c9-2913-4b4c-d461-71db3084f800",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 tokens: ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.']\n",
            "\n",
            "First 250 characters:\n",
            " [Moby Dick by Herman Melville 1851]\n",
            "\n",
            "\n",
            "ETYMOLOGY.\n",
            "\n",
            "(Supplied by a Late Consumptive Usher to a Grammar School)\n",
            "\n",
            "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
            "now.  He was ever dusting his old lexicons and grammars, with \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download the 'gutenberg' corpus, which is a collection of books in the public domain\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Get a plain-text version of Moby Dick (contained in a single string)\n",
        "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
        "\n",
        "# Get all the tokens in Moby Dick (as a list of strings)\n",
        "moby_tokens = gutenberg.words('melville-moby_dick.txt')\n",
        "\n",
        "# Print the first 10 tokens of Moby Dick\n",
        "print(\"First 10 tokens:\", moby_tokens[:10])\n",
        "\n",
        "# Print the first 250 characters of Moby Dick\n",
        "print(\"\\nFirst 250 characters:\\n\", moby_raw[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk7r17iXgZW-"
      },
      "source": [
        "NLTK includes a Text class for analyzing the contents of texts. Let's print a concordance for the word *Iceland*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCPcW0p9gZXB",
        "outputId": "3eeabb65-b35d-4c44-eef2-09fb40d2b74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying 4 of 4 matches:\n",
            "ANKS ' S AND SOLANDER ' S VOYAGE TO ICELAND IN 1772 . \" The Spermacetti Whale f\n",
            " an adjoining room . It was cold as Iceland -- no fire at all -- the landlord s\n",
            " ? Throw yourselves ! Legs ! legs ! ICELAND SAILOR . I don ' t like your floor \n",
            "my of Sciences setting down certain Iceland Whales ( reydan - siskur , or Wrink\n"
          ]
        }
      ],
      "source": [
        "moby_text = nltk.Text(moby_tokens)\n",
        "moby_text.concordance('Iceland')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntDr5ACsgZXM"
      },
      "source": [
        "NTLK offers several ways to segment text into sentences and tokenize it. Let's see how `nltk.sent_tokenize()` and `nltk.word_tokenize()` work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IZ1nLySgZXQ",
        "outputId": "db473746-17a3-490f-e040-f1ee50ec11bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 sentences:\n",
            ">>> [Moby Dick by Herman Melville 1851]\n",
            "\n",
            "\n",
            "ETYMOLOGY.\n",
            ">>> (Supplied by a Late Consumptive Usher to a Grammar School)\n",
            "\n",
            "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
            "now.\n",
            ">>> He was ever dusting his old lexicons and grammars, with a queer\n",
            "handkerchief, mockingly embellished with all the gay flags of all the\n",
            "known nations of the world.\n",
            ">>> He loved to dust his old grammars; it\n",
            "somehow mildly reminded him of his mortality.\n",
            ">>> \"While you take in hand to school others, and to teach them by what\n",
            "name a whale-fish is to be called in our tongue leaving out, through\n",
            "ignorance, the letter H, which almost alone maketh the signification\n",
            "of the word, you deliver that which is not true.\"\n",
            "\n",
            "Total number of sentences: 9,852\n",
            "\n",
            "Tokens: ['He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.']\n"
          ]
        }
      ],
      "source": [
        "# Download the Punkt tokenizer model\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "moby_sentences = nltk.sent_tokenize(moby_raw)  # Split raw text into sentences\n",
        "tokens = nltk.word_tokenize(moby_sentences[3])  # Split a string into tokens\n",
        "\n",
        "print(\"First 5 sentences:\")\n",
        "for sentence in moby_sentences[:5]:\n",
        "    print(\">>>\", sentence)\n",
        "\n",
        "print(f\"\\nTotal number of sentences: {len(moby_sentences):,}\")\n",
        "\n",
        "print(\"\\nTokens:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC_OW-lUgZXo"
      },
      "source": [
        "## Regular expressions\n",
        "The Python standard library includes the `re` module for handling regular expressions ([reference](https://docs.python.org/3/library/re.html)). In Python, regular expression patterns should be created using *raw* strings, which are prefixed with an `r`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cDMt90gZXs",
        "outputId": "23658205-1f61-4f64-93b9-35934ac20feb",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sovereignest',\n",
              " 'off--serenest',\n",
              " 'monstrousest',\n",
              " 'Wonderfullest',\n",
              " 'surrenderest',\n",
              " 'cowardly--quickest']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "re.findall(r'\\b\\S{9,}est\\b', moby_raw)\n",
        "#must contain at least 9 characters befor est"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z_417IcgZX1"
      },
      "source": [
        "You can capture text that matches a specific part of a pattern in a group by enclosing it within parentheses. When making substitutions with `re.sub()`, you can refer to these groups using `\\number` (e.g., `\\1` and `\\2`), where the number refers to their position in the pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aHyZVF5pgZX3",
        "outputId": "c1d2bb99-6abc-418f-c5b1-d5febabbdc67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The wrath of grapes'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "another_string = \"The grapes of wrath\"\n",
        "re.sub(r'(\\S+) of (\\S+)', r'\\2 of \\1', another_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-U35BXYgZYD"
      },
      "source": [
        "NLTK offers a simple way of searching for sequences of tokens using regular expressions, where tokens can be enclosed in angle brackets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRChhPklgZYH",
        "outputId": "90934b81-e03b-4b62-9baa-d2461522103d",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ship so swiftly sped; snatch some sweet solace\n"
          ]
        }
      ],
      "source": [
        "# First we must create a Text object from a list of tokens\n",
        "moby_text = nltk.Text(moby_tokens)\n",
        "\n",
        "# Let's search for sequences of four tokens which all begin with the letter S\n",
        "moby_text.findall(r'<[Ss].*>{4,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIt3E060gZYQ"
      },
      "source": [
        "You can use groups to target specific tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L3nrg2SgZYV",
        "outputId": "2301cfab-a5e3-4e4b-97b1-d4c733272ecb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full; gallant; whale; modern; trading; large; Nantucket; leaking;\n",
            "midnight; wrecked; occupied; fine; jolly; empty; great; large; leaky;\n",
            "Nantucket; full; full; empty; whole; large; sagacious\n"
          ]
        }
      ],
      "source": [
        "moby_text.findall(r'<[Aa]n?>(<.+>)<ship>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz_n_cwgZYb"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before midnight, August 30th. Make a copy of this notebook (`\"File\" > \"Save a copy in Drive\"`) and enter your solutions in the cells below each question. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcJJbIUgZYi"
      },
      "source": [
        "### Question 1\n",
        "Get the raw text of `carroll-alice.txt` (Alice in Wonderland) from the Gutenberg corpus in NLTK and tokenize it using `nltk.word_tokenize()`.\n",
        "\n",
        "1. How many tokens does it contain in total?\n",
        "2. How many unique tokens (types) does it contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsA5lgp0gZYx",
        "outputId": "e8e4bfda-1b33-4e76-d1c5-a026a286312a",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 34110\n",
            "Unique tokens: 3016\n",
            "Total tokens: 33535\n",
            "Unique tokens: 3157\n"
          ]
        }
      ],
      "source": [
        "#Using Gutenberg word\n",
        "alice_tokens = gutenberg.words('carroll-alice.txt')\n",
        "print(\"Total tokens:\", len(alice_tokens))\n",
        "print(\"Unique tokens:\", len(set(alice_tokens)))\n",
        "\n",
        "#Using word tokenize\n",
        "alice_raw = gutenberg.raw('carroll-alice.txt')\n",
        "alice_word_tokenizer  = nltk.word_tokenize(alice_raw)\n",
        "print(\"Total tokens:\", len(alice_word_tokenizer))\n",
        "print(\"Unique tokens:\", len(set(alice_word_tokenizer)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9OMFNxgZY6"
      },
      "source": [
        "### Question 2\n",
        "Use `nltk.FreqDist()` to create a frequency distribution of all the tokens in Alice in Wonderland. What are the 20 most frequently occurring tokens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOPA_dljgZY_",
        "outputId": "763da124-1c5a-4de8-b632-d94811d99a80",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(',', 1993),\n",
              " (\"'\", 1731),\n",
              " ('the', 1527),\n",
              " ('and', 802),\n",
              " ('.', 764),\n",
              " ('to', 725),\n",
              " ('a', 615),\n",
              " ('I', 543),\n",
              " ('it', 527),\n",
              " ('she', 509),\n",
              " ('of', 500),\n",
              " ('said', 456),\n",
              " (\",'\", 397),\n",
              " ('Alice', 396),\n",
              " ('in', 357),\n",
              " ('was', 352),\n",
              " ('you', 345),\n",
              " (\"!'\", 278),\n",
              " ('that', 275),\n",
              " ('as', 246)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.FreqDist(alice_tokens).most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ku3fBv6gZZH"
      },
      "source": [
        "### Question 3\n",
        "Use `nltk.sent_tokenize()` to segment Alice in Wonderland into sentences, then find the longest sentence in the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "-EuYbFqJgZZI",
        "outputId": "deef4a3e-2956-443f-cf19-a213a8e4d970",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hardly knowing what she did, she picked up a little bit of stick, and\\nheld it out to the puppy; whereupon the puppy jumped into the air off\\nall its feet at once, with a yelp of delight, and rushed at the stick,\\nand made believe to worry it; then Alice dodged behind a great thistle,\\nto keep herself from being run over; and the moment she appeared on the\\nother side, the puppy made another rush at the stick, and tumbled head\\nover heels in its hurry to get hold of it; then Alice, thinking it was\\nvery like having a game of play with a cart-horse, and expecting every\\nmoment to be trampled under its feet, ran round the thistle again; then\\nthe puppy began a series of short charges at the stick, running a very\\nlittle way forwards each time and a long way back, and barking hoarsely\\nall the while, till at last it sat down a good way off, panting, with\\nits tongue hanging out of its mouth, and its great eyes half shut.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alice_raw = gutenberg.raw('carroll-alice.txt')\n",
        "alice_sent = nltk.sent_tokenize(alice_raw)\n",
        "max(alice_sent, key=len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExlDi0jDgZZR"
      },
      "source": [
        "### Question 4\n",
        "Use a regular expression to find all tokens in Alice in Wonderland that contain an *x* and end with *ed*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtfpRxu5gZZT",
        "outputId": "fe5f175b-f776-4d6e-e8e1-a87b0612bbe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mixed',\n",
              " 'fixed',\n",
              " 'executed',\n",
              " 'expected',\n",
              " 'exclaimed',\n",
              " 'boxed',\n",
              " 'executed',\n",
              " 'executed',\n",
              " 'exclaimed',\n",
              " 'exclaimed',\n",
              " 'exclaimed',\n",
              " 'explained',\n",
              " 'exclaimed',\n",
              " 'executed',\n",
              " 'executed',\n",
              " 'executed',\n",
              " 'exclaimed',\n",
              " 'mixed']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.findall(r'\\S*x\\S*ed',alice_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0rG5i_VgZZa"
      },
      "source": [
        "### Bonus Question\n",
        "\n",
        "Use `re.sub()` to \"dehyphenate\" the following string:\n",
        "\n",
        ">It is a capital mistake to theo-  \n",
        ">rize before one has data. Insen-  \n",
        ">sibly one begins to twist facts  \n",
        ">to suit theories, instead of the-  \n",
        ">ories to suit facts.\n",
        "\n",
        "You will need to use groups to recombine the words. The resulting string should look like this:\n",
        "\n",
        ">It is a capital mistake to  \n",
        ">theorize before one has data.  \n",
        ">Insensibly one begins to twist facts  \n",
        ">to suit theories, instead of  \n",
        ">theories to suit facts.\n",
        "\n",
        "Remember that a \"newline\" character is represented by `\\n` in strings and regular expression patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKyWgOPkgZZe",
        "outputId": "74c3b8d5-6f47-4b7d-d599-7ecef78913f1",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "It is a capital mistake to \n",
            "theorize before one has data. \n",
            "Insensibly one begins to twist facts\n",
            "to suit theories, instead of \n",
            "theories to suit facts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your solution here\n",
        "hyphenated = \"\"\"\n",
        "It is a capital mistake to theo-\n",
        "rize before one has data. Insen-\n",
        "sibly one begins to twist facts\n",
        "to suit theories, instead of the-\n",
        "ories to suit facts.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(re.sub(r'(\\S+)-\\n(\\S+)', r'\\n\\1\\2',hyphenated))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
