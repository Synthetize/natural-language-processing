{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pajD7caf9pX7"
      },
      "source": [
        "# T-725 Natural Language Processing: Lab 2\n",
        "In today's lab, we will be working with text classification.\n",
        "\n",
        "To begin with, do the following:\n",
        "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
        "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsSxs1XKhi_4"
      },
      "source": [
        "## List comprehensions in Python\n",
        "List comprehensions are a concise way of creating lists in Python, and take the form:\n",
        "\n",
        "```python\n",
        "[expression for item in iterable]\n",
        "```\n",
        "\n",
        "A list comprehension creates a new list by evaluating some expression for every item in a given iterable (such as a string, a list or a dictionary). Let's look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxB1Ip77hzFL",
        "outputId": "4bd9d8d5-2eed-463b-cc02-5ef623ba7152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence = \"In a hole in the ground there lived a hobbit.\"\n",
        "words = sentence.split()\n",
        "print(words)\n",
        "\n",
        "# Example of a list comprehension\n",
        "word_lengths = [len(word) for word in words]\n",
        "print(word_lengths)\n",
        "\n",
        "# This is equal to\n",
        "word_lengths = []\n",
        "for word in words:\n",
        "  word_lengths.append(len(word))\n",
        "\n",
        "print(word_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit.']\n",
            "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n",
            "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWw6IRBgkQbc"
      },
      "source": [
        "You can also add a conditional statement to list comprehensions, so that the expression will only be evaluated for items that meet a certain criteria:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSRG8kBdjZKA",
        "outputId": "7e113c2c-3f5f-4578-83aa-e4436dbb43a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "e_words = [word for word in words if len(word) > 5]\n",
        "print(e_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ground', 'hobbit.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTTJDJ5qZML"
      },
      "source": [
        "Python also has set and dictionary comprehensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniqHWPoqd8E",
        "outputId": "986c80af-1fbf-4f7f-e652-7199df8f43af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lowercase_characters = {c.lower() for c in sentence}\n",
        "print(lowercase_characters)\n",
        "\n",
        "word_length = {word: len(word) for word in words}\n",
        "print(word_length['ground'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ', '.', 'h', 'b', 't', 'a', 'd', 'n', 'i', 'o', 'l', 'u', 'e', 'r', 'v', 'g'}\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3Y_4kIbpaU"
      },
      "source": [
        "A nested list is a list within another list. You can iterate through nested lists in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW8w46PbuFM",
        "outputId": "d5629e45-504c-4c30-e3e2-78c622504467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# A list of countries and their capitals within different continents\n",
        "continents = [\n",
        "    [('Iceland', 'Reykjav√≠k'), ('Germany', 'Berlin'), ('Spain', 'Madrid')],  # Europe\n",
        "    [('Japan', 'Tokyo'), ('China', 'Beijing'), ('South Korea', 'Seoul')],  # Asia\n",
        "    [('Nigeria', 'Abuja'), ('Algeria', 'Algiers'), ('Angola', 'Luanda')]  # Africa\n",
        "]\n",
        "\n",
        "# Create a list of all the countries in the previous list\n",
        "[country for continent in continents for (country, capital) in continent]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Iceland',\n",
              " 'Germany',\n",
              " 'Spain',\n",
              " 'Japan',\n",
              " 'China',\n",
              " 'South Korea',\n",
              " 'Nigeria',\n",
              " 'Algeria',\n",
              " 'Angola']"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBrOT1vNcYZ0"
      },
      "source": [
        "## Sentiment analysis with NLTK\n",
        "[Chapter 6](https://www.nltk.org/book/ch06.html) of the NLTK book shows how the toolkit can be used to create document classifiers, including a sentiment analyzer. The NLTK includes the `movie_reviews` corpus, which contains 2,000 movie reviews. Half of the reviews have been labelled as **positive** and the other half as **negative**. Let's download it and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx01nR8x9qgq",
        "outputId": "8bccdaa3-7c0c-4f08-b377-57b94de0e33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "print(\"Categories:\", movie_reviews.categories())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: ['neg', 'pos']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AppRxScjc9vx"
      },
      "source": [
        "As expected, there are two categories: `pos` for positive reviews and `neg` for negative reviews. For this particular corpus, each review is stored as a separate text file. To get a list of all the text files in the corpus, we can use `movie_reviews.fileids()`. We can also get a list of files for a specific category:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI8EjJoo_1jz",
        "outputId": "f9324c00-7446-4dc6-fba1-a7bf6d94945f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_fileids = movie_reviews.fileids('pos')\n",
        "neg_fileids = movie_reviews.fileids('neg')\n",
        "\n",
        "print(pos_fileids[:5])  # The first 5 positive reviews\n",
        "print(neg_fileids[:5])  # The first 5 negative reviews"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt']\n",
            "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_C9_80AEDo"
      },
      "source": [
        "We can get a list of all the tokens in the corpus with `movie_reviews.words()`. We can also specify a filename to get a single tokenized review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8mPNPGcnbN",
        "outputId": "edbb9dff-af1d-46dc-ec48-281daa20f183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_reviews = [movie_reviews.words(fid) for fid in pos_fileids]\n",
        "neg_reviews = [movie_reviews.words(fid) for fid in neg_fileids]\n",
        "\n",
        "print(pos_reviews[0][:10])  # The first 10 tokens of the first positive review\n",
        "print(neg_reviews[0][:10])  # The first 10 tokens of the first negative review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04vosHorN6F"
      },
      "source": [
        "Some words, such as *brilliant* and *memorable*, are more strongly associated with positive reviews than negative ones. Similarly, *boring* and *unfunny* have a stronger association with negative reviews.\n",
        "\n",
        "Using the movie review corpus, we can train a classifier to predict whether a given review is positive or negative. The classifier extracts a set of *features* from every review, which are then used to make the classification. In this case, the features we use will be a dictionary that tells us whether each of the 2,000 most common words in the corpus is present within a review or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeixOJ-lu1fD"
      },
      "source": [
        "# Create a set with 2,000 of the most frequent words in the movie review corpus\n",
        "movie_fd = nltk.FreqDist(movie_reviews.words())\n",
        "movie_words = {word for word, count in movie_fd.most_common(2000)}\n",
        "\n",
        "# For a given review (in the form of a list or set of tokens), create a\n",
        "# dictionary which tells us which words are present and which are not.\n",
        "def get_review_features(review):\n",
        "  review_words = set(review)\n",
        "  return {word: word in review_words for word in movie_words}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-r6kXma8-eB",
        "outputId": "d9c83c69-1990-41cd-a894-dea310576444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's see how this works for the first positive review:\n",
        "example_features = get_review_features(pos_reviews[0])\n",
        "print(\"'funny' is in the review:\", example_features['funny'])\n",
        "print(\"'boring' is in the review:\", example_features['boring'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'funny' is in the review: True\n",
            "'boring' is in the review: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5rLkC7TCVC"
      },
      "source": [
        "Next, let's create a training set that we can use to train a Naive Bayesian classifier. The training set, in this case, is a list of tuples in the format `[(features, category), ...]`, where `features` is a dictionary from `get_review_features()` and `category` is either `pos` or `neg`, depending on whether the review is positive or negative. To get an idea of how well the classifier performs, we're going to reserve 10% of the reviews for testing. That means that we'll be training our classifier on 1800 examples and testing it on 200 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqhiywiwrhxW"
      },
      "source": [
        "pos_examples = [(get_review_features(review), 'pos') for review in pos_reviews]\n",
        "neg_examples = [(get_review_features(review), 'neg') for review in neg_reviews]\n",
        "\n",
        "movie_training = pos_examples[:900] + neg_examples[:900]  # 1800 examples total\n",
        "movie_test = pos_examples[900:] + neg_examples[900:]  # 200 examples total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfty2sigVuf3"
      },
      "source": [
        "Now we have everything we need to train our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAqq1Z72wLHl"
      },
      "source": [
        "movie_classifier = nltk.NaiveBayesClassifier.train(movie_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3inpPaRDV0_6"
      },
      "source": [
        "How well does it perform on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eJMkqdw7Eb",
        "outputId": "825097be-462f-464d-c516-1e22cb19fc22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Accuracy:\", nltk.classify.accuracy(movie_classifier, movie_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxakIFH5WNH_"
      },
      "source": [
        "The classifier achieves an accuracy of 81.5%. Let's take a look at which words have the biggest weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibMPC0QCxefo",
        "outputId": "503366a0-1045-4c89-ef67-e7041718ed28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "movie_classifier.show_most_informative_features(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             outstanding = True              pos : neg    =     15.6 : 1.0\n",
            "                   mulan = True              pos : neg    =      9.0 : 1.0\n",
            "             wonderfully = True              pos : neg    =      7.1 : 1.0\n",
            "                  seagal = True              neg : pos    =      7.0 : 1.0\n",
            "                   damon = True              pos : neg    =      6.1 : 1.0\n",
            "                   flynt = True              pos : neg    =      5.7 : 1.0\n",
            "                  wasted = True              neg : pos    =      5.6 : 1.0\n",
            "                    lame = True              neg : pos    =      5.3 : 1.0\n",
            "                  poorly = True              neg : pos    =      5.2 : 1.0\n",
            "                   awful = True              neg : pos    =      4.9 : 1.0\n",
            "              ridiculous = True              neg : pos    =      4.8 : 1.0\n",
            "                    jedi = True              pos : neg    =      4.4 : 1.0\n",
            "                 unfunny = True              neg : pos    =      4.4 : 1.0\n",
            "                   waste = True              neg : pos    =      4.4 : 1.0\n",
            "               fantastic = True              pos : neg    =      4.4 : 1.0\n",
            "                   worst = True              neg : pos    =      4.2 : 1.0\n",
            "                    mess = True              neg : pos    =      4.2 : 1.0\n",
            "                  stupid = True              neg : pos    =      4.2 : 1.0\n",
            "                   bland = True              neg : pos    =      4.0 : 1.0\n",
            "                     era = True              pos : neg    =      4.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPSLZUxt9uMY"
      },
      "source": [
        "# Assignment\n",
        "Answer the following questions and hand in your solution in Canvas before 23:59 on September 5th. Remember to save your file before uploading it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQN5Qq9z97Yc"
      },
      "source": [
        "## Question 1\n",
        "The NLTK also includes a `subjectivity` corpus, which contains a collection of sentences that have either been categorized as **subjective** (emotional, expressing personal feelings and views)  or **objective** (more rational, factual). Some examples:\n",
        "\n",
        "* **Objective sentences**:\n",
        "  * uma thurman stars in quentin tarantino's fourth film venture , kill bill .  \n",
        "  * he lives in a motor garage with his six friends .\n",
        "  * the ensuing battle was one of the most savage in u . s . history .\n",
        "* **Subjective sentences**:\n",
        "  * seagal's strenuous attempt at a change in expression could very well clinch him this year's razzie .\n",
        "  * de niro cries . you'll cry for your money back .\n",
        "  * a heroic tale of persistence that is sure to win viewers' hearts .\n",
        "\n",
        "Unlike the movie review corpus, where every review is stored in separate file, here there is only one file for each category.\n",
        "\n",
        "Complete the following tasks:\n",
        "1. Import and download the `subjectivity` corpus.\n",
        "2. Find the names of each category.\n",
        "3. Using the category names, get the relative path of each file.\n",
        "4. Get a list of tokenized sentences for each category (using `subjectivity.sents(fileid)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc1EM7TvnYaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f24161-bf47-4608-f18b-6d4eb2d774bc"
      },
      "source": [
        "from nltk.corpus import subjectivity\n",
        "\n",
        "nltk.download(\"subjectivity\")\n",
        "print(\"Categories: \", subjectivity.categories())\n",
        "\n",
        "obj_fileid = subjectivity.fileids('obj')\n",
        "subj_fileid = subjectivity.fileids('subj')\n",
        "\n",
        "obj_tokenized_sents = subjectivity.sents(obj_fileid)\n",
        "subj_tokenized_sents = subjectivity.sents(subj_fileid)\n",
        "\n",
        "print(\"Objectives: \", obj_tokenized_sents[:5])\n",
        "print(\"Subjective: \", subj_tokenized_sents[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories:  ['obj', 'subj']\n",
            "Objectives:  [['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ['spurning', 'her', \"mother's\", 'insistence', 'that', 'she', 'get', 'on', 'with', 'her', 'life', ',', 'mary', 'is', 'thrown', 'out', 'of', 'the', 'house', ',', 'rejected', 'by', 'joe', ',', 'and', 'expelled', 'from', 'school', 'as', 'she', 'grows', 'larger', 'with', 'child', '.'], ['amitabh', \"can't\", 'believe', 'the', 'board', 'of', 'directors', 'and', 'his', 'mind', 'is', 'filled', 'with', 'revenge', 'and', 'what', 'better', 'revenge', 'than', 'robbing', 'the', 'bank', 'himself', ',', 'ironic', 'as', 'it', 'may', 'sound', '.'], ['she', ',', 'among', 'others', 'excentricities', ',', 'talks', 'to', 'a', 'small', 'rock', ',', 'gertrude', ',', 'like', 'if', 'she', 'was', 'alive', '.']]\n",
            "Subjective:  [['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ['it', 'is', 'not', 'a', 'mass-market', 'entertainment', 'but', 'an', 'uncompromising', 'attempt', 'by', 'one', 'artist', 'to', 'think', 'about', 'another', '.'], ['a', 'light-hearted', 'french', 'film', 'about', 'the', 'spiritual', 'quest', 'of', 'a', 'fashion', 'model', 'seeking', 'peace', 'of', 'mind', 'while', 'in', 'a', 'love', 'affair', 'with', 'a', 'veterinarian', 'who', 'is', 'a', 'non-practicing', 'jew', '.'], ['my', 'wife', 'is', 'an', 'actress', 'has', 'its', 'moments', 'in', 'looking', 'at', 'the', 'comic', 'effects', 'of', 'jealousy', '.', 'in', 'the', 'end', ',', 'though', ',', 'it', 'is', 'only', 'mildly', 'amusing', 'when', 'it', 'could', 'have', 'been', 'so', 'much', 'more', '.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaXJ-5Lr97_w"
      },
      "source": [
        "## Question 2\n",
        "Complete the following tasks:\n",
        "1. Create a set with the 2,000 most common words in the `subjectivity` corpus using `nltk.FreqDist()`.\n",
        "2. Create a function that takes a single, tokenized sentence as input (e.g., `['the', 'ensuing', 'battle', ...]`), and returns a dictionary of the 2,000 most frequent words and whether or not they are in the sentence (e.g., `{'battle': True, 'amusing': False, ...}`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bx4kZSW98xq"
      },
      "source": [
        "words = subjectivity.words()\n",
        "common_tokens = nltk.FreqDist(words).most_common(2000) #tuple sequence\n",
        "common_tokens = {word for (word, count) in common_tokens} #transform the tuple in a list of words\n",
        "\n",
        "def frequent_words(sentence):\n",
        "  set_sentence = set(sentence)\n",
        "  return {word: word in set_sentence for word in common_tokens}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrxhu0iB99YN"
      },
      "source": [
        "## Question 3\n",
        "Complete the following tasks:\n",
        "1. Create a training set with 9,000 sentences (4,500 of each category)\n",
        "2. Create a test set with 1,000 sentences (500 of each category)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xWq_S899xK"
      },
      "source": [
        "obj_samples = [(frequent_words(sent), \"obj\") for sent in obj_tokenized_sents]\n",
        "subj_samples = [(frequent_words(sent), \"subj\") for sent in subj_tokenized_sents]\n",
        "\n",
        "train_sample = obj_samples[:4500] + subj_samples[:4500]\n",
        "test_sample = obj_samples[4500:] + subj_samples[4500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HToPuGzX9-Cl"
      },
      "source": [
        "## Question 4\n",
        "Complete the following tasks:\n",
        "1. Train a Naive Bayes classifier using the training set from the previous question.\n",
        "2. Evaluate the classifier on the test set. How accurate is it?\n",
        "3. Find the 20 most informative features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrN1tvKZa4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd90caf-4489-469f-a515-eaaa0ea17d7f"
      },
      "source": [
        "sentences_classifier = nltk.NaiveBayesClassifier.train(train_sample)\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(sentences_classifier, test_sample))\n",
        "sentences_classifier.show_most_informative_features(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.906\n",
            "Most Informative Features\n",
            "                      -- = True             subj : obj    =     70.1 : 1.0\n",
            "                   order = True              obj : subj   =     39.0 : 1.0\n",
            "                 decides = True              obj : subj   =     35.7 : 1.0\n",
            "                  sister = True              obj : subj   =     27.7 : 1.0\n",
            "            entertaining = True             subj : obj    =     26.6 : 1.0\n",
            "              girlfriend = True              obj : subj   =     26.3 : 1.0\n",
            "                discover = True              obj : subj   =     25.0 : 1.0\n",
            "                  film's = True             subj : obj    =     25.0 : 1.0\n",
            "                  you're = True             subj : obj    =     22.6 : 1.0\n",
            "                daughter = True              obj : subj   =     22.4 : 1.0\n",
            "                 married = True              obj : subj   =     21.7 : 1.0\n",
            "                 amusing = True             subj : obj    =     19.7 : 1.0\n",
            "                   plans = True              obj : subj   =     19.7 : 1.0\n",
            "                probably = True             subj : obj    =     19.7 : 1.0\n",
            "                    plan = True              obj : subj   =     19.4 : 1.0\n",
            "                    town = True              obj : subj   =     19.3 : 1.0\n",
            "                  you've = True             subj : obj    =     19.0 : 1.0\n",
            "                    kill = True              obj : subj   =     18.6 : 1.0\n",
            "                    slow = True             subj : obj    =     18.3 : 1.0\n",
            "             interesting = True             subj : obj    =     18.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5\n",
        "Dialog acts are sort of the type of *action* performed by the speaker. In the instant messaging corpus dataset 'NPS', each utterance is labeled with one of 15 dialogue act types, such as **Statement**, **Emotion**, **ynQuestion**, **Continuer**, etc.\n",
        "\n",
        "Your task is to classify text from the NPS corpus into two dialog acts: **whQuestion** or **Emotion**.\n",
        "\n",
        "Start by downloading the NPS corpus and getting all posts from the corpus:"
      ],
      "metadata": {
        "id": "f-J9BEEQFrbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('nps_chat')\n",
        "posts = nltk.corpus.nps_chat.xml_posts()"
      ],
      "metadata": {
        "id": "Qtc_pG1qJZgh",
        "outputId": "facfc7e7-b5a2-4f15-a202-89ea3a12bf6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list that only includes posts of class **Emotion** and **whQuestion**. You can access the class of a post by calling `post.get(\"class\")`."
      ],
      "metadata": {
        "id": "p0wQn19UqfKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_posts = [post for post in posts if post.get(\"class\") == \"Emotion\"]\n",
        "whQuestions_posts = [post for post in posts if post.get(\"class\") == \"whQuestion\"]\n",
        "needed_posts = emotions_posts + whQuestions_posts\n",
        "\n"
      ],
      "metadata": {
        "id": "H316uXLTrRUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the posts and create a training set and a test set, where the first 1300 **Emotion + whQuestion** posts are used for training and the rest for testing."
      ],
      "metadata": {
        "id": "t1K-jkXCzv4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(needed_posts)\n",
        "\n",
        "train_set = needed_posts[:1300]\n",
        "test_set = needed_posts[1300:]\n"
      ],
      "metadata": {
        "id": "1SQdx2rY0LG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list of the 200 most frequent tokens in the training set. You can access the text of a `post` object by calling `post.text`. Remember that the **split** function will use whitespace to tokenize a string: `some_string.split()`"
      ],
      "metadata": {
        "id": "38Klc5bHxDvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_posts = [post.text.lower().split() for post in train_set]\n",
        "all_posts_words = [word for post in tokenized_posts for word in post]\n",
        "frequent_tokens = nltk.FreqDist(all_posts_words).most_common(200)\n",
        "frequent_tokens = [word for word, count in frequent_tokens]\n",
        "print(frequent_tokens)\n"
      ],
      "metadata": {
        "id": "cmBZL8wxso-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9f0d46-15b2-4707-a695-47d3bde99762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lol', 'you', 'what', 'how', 'lmao', 'the', 'are', 'who', 'is', 'to', 'u', 'haha', 'whats', 'and', 'why', ':)', 'where', 'in', 'up', 'did', 'do', 'i', 'a', 'that', 'it', 'omg', 'so', 'about', 'was', 'for', 'of', 'here', '?', 'oh', ';)', ';-)', 'all', 'your', 'damn', '11-09-40suser18', 'r', 'everyone', 'ok', 'on', 'good', 'me', 'hahaha', '11-08-adultsuser65', ':-)', 'with', \"what's\", ':p', 'when', 'but', 'you?', 'from', 'my', 'have', 'come', 'what?', 'doing', '10-19-40suser9', 'hi', 'ya', 'wants', 'from?', '10-19-adultsuser35', \"who's\", '@', '10-19-adultsuser23', 'or', 'been', 'like', 'her', 'ha', 'girl', '10-19-30suser11', 'at', '11-08-20suser21', '10-19-40suser3', '10-19-20suser7', 'o.o', ':tongue:', 'he', '???', 'hows', 'there', '..', 'which', '10-19-adultsuser28', 'happened', '11-09-40suser7', 'we', 'doin', 'yes', 'hahah', '10-19-20suser121', 'lmfao', 'name', 'does', 'get', 'laffs', '10-19-30suser31', '10-19-adultsuser47', ':-o', 'know', '11-06-adultsuser105', '((((((((((', 'can', 'am', '11-08-adultsuser3', '11-09-40suser31', '10-19-40suser34', 'im', 'asl', 'if', 'hell', 'fine', 'should', 'aw', 'hugs', '10-19-30suser19', 'o0', 'same', '10-19-adultsuser32', 'say', 'this', 'talk', '11-09-40suser48', 'not', '10-24-40suser41', 'round', 'go', 'wtf', 'they', 'were', '.', '11-09-adultsuser72', 'day', 'girls', 'song', 'red', 'rofl', '11-08-40suser48', 'tonight?', 'many', 'nick', '10-24-40suser13', 'him', 'really', 'feel', 'why?', 'yay!', 'huh?', 'just', 'want', 'whos', '10-19-20suser115', ':o', 'awww', 'me?', 'no', '11-09-20suser156', 'where?', 'much', 'lol....', 'hey', 'goin', ')))))))))))))', '11-09-40suser49', 'part', '10-19-30suser28', '10-24-40suser6', 'wow', 'that?', '10-19-adultsuser1', '10-24-40suser34', 'hah', 'o', 'shit', 'lmaooo', 'today?', 'ur', 'lmaoo', 'cool', 'going', 'room', '10-24-40suser19', 'be', 'well', 'ty', '??', 'wanna', 'pm', 'talking', '<3', '11-06-adultsuser53', 'eww', 'doin?', 'awwwwwwwwww']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define two feature selection functions that take a string as input and output a dictionary of features:\n",
        "* `get_word_features(string)`\n",
        "* `get_custom_features(string)`\n",
        "\n",
        "Begin by defining `get_word_features`. This function should use the words as features, just like in the movie review example above.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tyC-0es9KwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_features(string):\n",
        "  tokenized_string = set(string.lower().split())\n",
        "  return {word: word in tokenized_string for word in frequent_tokens}\n"
      ],
      "metadata": {
        "id": "6b6mKRLiFqEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define `get_custom_features`. This function should extract the features from the text that characterize the **Emotion** and **whQuestions** classes."
      ],
      "metadata": {
        "id": "n4LmO-UaJCHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#return the \"count\" number of most frequent tokens of a spcific class in a post list\n",
        "def common_tokens_by_class(post_list, post_class, count):\n",
        "  class_posts = [post for post in post_list if post.get(\"class\") == post_class]\n",
        "  tokenized_list = [post.text.lower().split() for post in class_posts]\n",
        "  all_class_tokens = [word for post in tokenized_list for word in post]\n",
        "  return {word for word,_ in nltk.FreqDist(all_class_tokens).most_common(count)}\n",
        "\n",
        "#find the most common tokens for emotion and whquestion and join them togheter\n",
        "emotion_common_tokens = common_tokens_by_class(posts, \"Emotion\", 30)\n",
        "whquestion_common_token = common_tokens_by_class(posts, \"whQuestion\", 30)\n",
        "custom_common_tokens = set(emotion_common_tokens | whquestion_common_token)\n",
        "\n",
        "def get_custom_features(string):\n",
        "  tokens_list = set(string.lower().split())\n",
        "  return {word: word in tokens_list for word in custom_common_tokens}\n",
        "\n",
        "get_custom_features(\"this is a sentence lol haha\")"
      ],
      "metadata": {
        "id": "w9CBUttQKEzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36df8cd0-b106-4133-b13a-0d1d49daabad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'about': False,\n",
              " 'hehe': False,\n",
              " 'what': False,\n",
              " 'of': False,\n",
              " '11-09-40suser31': False,\n",
              " '10-19-40suser9': False,\n",
              " 'who': False,\n",
              " 'a': True,\n",
              " ';)': False,\n",
              " 'did': False,\n",
              " 'how': False,\n",
              " 'you': False,\n",
              " 'where': False,\n",
              " 'it': False,\n",
              " ':tongue:': False,\n",
              " ';-)': False,\n",
              " 'oh': False,\n",
              " 'hahah': False,\n",
              " '10-19-40suser3': False,\n",
              " 'for': False,\n",
              " 'why': False,\n",
              " 'the': False,\n",
              " '?': False,\n",
              " 'lmao': False,\n",
              " 'up': False,\n",
              " 'haha': True,\n",
              " ':)': False,\n",
              " '11-08-adultsuser65': False,\n",
              " '11-08-20suser21': False,\n",
              " 'r': False,\n",
              " 'is': True,\n",
              " '10-19-adultsuser47': False,\n",
              " 'hahaha': False,\n",
              " 'ha': False,\n",
              " 'damn': False,\n",
              " 'omg': False,\n",
              " 'good': False,\n",
              " 'her': False,\n",
              " 'that': False,\n",
              " ':-o': False,\n",
              " 'i': False,\n",
              " 'in': False,\n",
              " ':p': False,\n",
              " 'u': False,\n",
              " '10-19-20suser7': False,\n",
              " 'are': False,\n",
              " ':-)': False,\n",
              " 'so': False,\n",
              " 'your': False,\n",
              " 'whats': False,\n",
              " '@': False,\n",
              " 'was': False,\n",
              " 'here': False,\n",
              " 'to': False,\n",
              " 'and': False,\n",
              " '((((((((((': False,\n",
              " '11-09-40suser18': False,\n",
              " 'do': False,\n",
              " 'lol': True}"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct the following tasks:\n",
        "*   Train two Naive Bayes classifiers on the **Emotion + whQuestions** training set: one that uses the `get_word_features` function and another using `get_custom_features`.\n",
        "*   Evaluate each classifier on the test set. How accurate are they? Which one is better?\n",
        "*   What are the 20 most informative features for each classifier?\n"
      ],
      "metadata": {
        "id": "KxGa5GS1J3aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model train using get_word_feature function\n",
        "emotion_train = [(get_word_features(sentence.text), sentence.get(\"class\")) for sentence in train_set if sentence.get(\"class\") == \"Emotion\"]\n",
        "whQuestion_train = [(get_word_features(sentence.text), sentence.get(\"class\")) for sentence in train_set if sentence.get(\"class\") == \"whQuestion\"]\n",
        "\n",
        "labeled_train = emotion_train + whQuestion_train\n",
        "random.shuffle(labeled_train)\n",
        "\n",
        "emotion_test = [(get_word_features(sentence.text), sentence.get(\"class\")) for sentence in test_set if sentence.get(\"class\") == \"Emotion\"]\n",
        "whQuestion_test = [(get_word_features(sentence.text), sentence.get(\"class\")) for sentence in test_set if sentence.get(\"class\") == \"whQuestion\"]\n",
        "labeled_test = emotion_test + whQuestion_test\n",
        "random.shuffle(labeled_test)\n",
        "\n",
        "first_classification = nltk.NaiveBayesClassifier.train(labeled_train)\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(first_classification, labeled_test))\n",
        "first_classification.show_most_informative_features(20)\n",
        "\n",
        "\n",
        "\n",
        "#model train using get_custom_features function\n",
        "emotion_train = [(get_custom_features(sentence.text), sentence.get(\"class\")) for sentence in train_set if sentence.get(\"class\") == \"Emotion\"]\n",
        "whQuestion_train = [(get_custom_features(sentence.text), sentence.get(\"class\")) for sentence in train_set if sentence.get(\"class\") == \"whQuestion\"]\n",
        "\n",
        "labeled_train = emotion_train + whQuestion_train\n",
        "random.shuffle(labeled_train)\n",
        "\n",
        "emotion_test = [(get_custom_features(sentence.text), sentence.get(\"class\")) for sentence in test_set if sentence.get(\"class\") == \"Emotion\"]\n",
        "whQuestion_test = [(get_custom_features(sentence.text), sentence.get(\"class\")) for sentence in test_set if sentence.get(\"class\") == \"whQuestion\"]\n",
        "labeled_test = emotion_test + whQuestion_test\n",
        "random.shuffle(labeled_test)\n",
        "\n",
        "second_classification = nltk.NaiveBayesClassifier.train(labeled_train)\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(second_classification, labeled_test))\n",
        "second_classification.show_most_informative_features(20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5cpU0eCg4RN",
        "outputId": "9af670b1-3b81-4110-f2b5-ec44519d206e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9321533923303835\n",
            "Most Informative Features\n",
            "                     how = True           whQues : Emotio =     94.6 : 1.0\n",
            "                       u = True           whQues : Emotio =     50.4 : 1.0\n",
            "                     you = True           whQues : Emotio =     34.7 : 1.0\n",
            "                     and = True           whQues : Emotio =     32.4 : 1.0\n",
            "                     the = True           whQues : Emotio =     29.9 : 1.0\n",
            "                      in = True           whQues : Emotio =     26.9 : 1.0\n",
            "                     lol = True           Emotio : whQues =     24.3 : 1.0\n",
            "                    that = True           whQues : Emotio =     21.4 : 1.0\n",
            "                    lmao = True           Emotio : whQues =     12.3 : 1.0\n",
            "                      to = True           whQues : Emotio =     11.5 : 1.0\n",
            "                      me = True           whQues : Emotio =     10.4 : 1.0\n",
            "                      so = True           whQues : Emotio =     10.4 : 1.0\n",
            "                   doing = True           whQues : Emotio =      9.0 : 1.0\n",
            "                    from = True           whQues : Emotio =      9.0 : 1.0\n",
            "                      hi = True           whQues : Emotio =      9.0 : 1.0\n",
            "                      it = True           whQues : Emotio =      7.4 : 1.0\n",
            "                     for = True           whQues : Emotio =      6.2 : 1.0\n",
            "                      he = True           whQues : Emotio =      6.2 : 1.0\n",
            "                   there = True           whQues : Emotio =      6.2 : 1.0\n",
            "                    haha = True           Emotio : whQues =      5.9 : 1.0\n",
            "Accuracy: 0.9203539823008849\n",
            "Most Informative Features\n",
            "                     how = True           whQues : Emotio =     94.6 : 1.0\n",
            "                       u = True           whQues : Emotio =     50.4 : 1.0\n",
            "                     you = True           whQues : Emotio =     34.7 : 1.0\n",
            "                     and = True           whQues : Emotio =     32.4 : 1.0\n",
            "                     the = True           whQues : Emotio =     29.9 : 1.0\n",
            "                      in = True           whQues : Emotio =     26.9 : 1.0\n",
            "                     lol = True           Emotio : whQues =     24.3 : 1.0\n",
            "                    that = True           whQues : Emotio =     21.4 : 1.0\n",
            "                    lmao = True           Emotio : whQues =     12.3 : 1.0\n",
            "                      to = True           whQues : Emotio =     11.5 : 1.0\n",
            "                      so = True           whQues : Emotio =     10.4 : 1.0\n",
            "                      it = True           whQues : Emotio =      7.4 : 1.0\n",
            "                     for = True           whQues : Emotio =      6.2 : 1.0\n",
            "                    haha = True           Emotio : whQues =      5.9 : 1.0\n",
            "                    here = True           whQues : Emotio =      5.6 : 1.0\n",
            "                       i = True           whQues : Emotio =      5.1 : 1.0\n",
            "                     omg = True           Emotio : whQues =      4.7 : 1.0\n",
            "                      ;) = True           Emotio : whQues =      3.1 : 1.0\n",
            "                     :-) = True           Emotio : whQues =      2.4 : 1.0\n",
            "                      oh = True           Emotio : whQues =      1.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWgyfpLjg46c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}